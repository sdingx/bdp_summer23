{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.appName('YelpML').getOrCreate()\n",
    "\n",
    "#change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "\n",
    "#print spark configuration settings\n",
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/12 02:29:52 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "dataDir = \"gs://msca-bdp-student-gcs/group2/yelp-datasample2\"\n",
    "business = spark.read.json(dataDir + \"/sample_business\")\n",
    "checkin = spark.read.json(dataDir + \"/sample_checkin\")\n",
    "review = spark.read.json(dataDir + \"/sample_review\")\n",
    "tip = spark.read.json(dataDir + \"/sample_tip\")\n",
    "user = spark.read.json(dataDir + \"/sample_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='arKiXax3ScSM_z3O-0CIyw', cool=0, date='2010-10-17 01:50:46', funny=0, review_id='zCNdcNrkIKefTPbak7CHVA', stars=5.0, text='Great Italian food!!  We have eaten here several times now and each time we have eaten something different.  Everytime the food has been fabulous!  We actually crave their food during the week and want to head over to Philadelphia for our Spasso food fix!', useful=0, user_id='bz2FrqfKrVmS7WwC-7C9aA')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "|business_id|cool|date|funny|review_id|stars|text|useful|user_id|\n",
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "|          0|   0|   0|    0|        0|    0|   0|     0|      0|\n",
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "review.select([count(when(review[c].isNull(), c)).alias(c) for c in review.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = review.sample(fraction=0.02, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1482"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|E03HQDIBBR1UHVd0B...|   0|2020-06-04 17:25:44|    0|Vnfrt7BhWVwBH4Zjq...|  5.0|The experience at...|     0|V8dN1Nvj8bKJ2GL8C...|\n",
      "|8_VaLzyX-H0nzbFIK...|   1|2014-12-03 00:25:46|    0|vJvjgW8cYExb2VM1o...|  5.0|It truly is as go...|     1|BkMqpJikNc3r5itc-...|\n",
      "|fTZZih-F-0VPbnv7H...|   0|2020-07-06 05:41:35|    0|QkSPQf4YCPg1Fs8s5...|  1.0|$20 a piece for a...|     0|G7Sabx-ak70f_Zt8O...|\n",
      "|Mfss88nOCGdyHkZZC...|   0|2014-03-07 23:37:04|    0|W8xEZ7SuEEaFwMdR6...|  4.0|I went to Caffe N...|     2|8MkZ6bpdP7x8Vlm_u...|\n",
      "|FgnNJt32BcXz7u4Ny...|   0|2020-06-10 16:43:40|    0|SJX5aSGfGJqt8PB5e...|  1.0|Disgusting custom...|     1|nVis9KXdsSyXLHFgJ...|\n",
      "|iwyMYJxTnwKLKBlOi...|   0|2021-04-14 00:23:51|    0|dsjGDOOJJdMKp7PvC...|  1.0|This is what I ge...|     1|ZHfo-Wq1DCcKvnsis...|\n",
      "|2BMk_drsikKWslJCX...|   0|2019-03-16 16:22:27|    0|lGGbrVGMbH2Ao2oGt...|  1.0|This was not a go...|     0|kMTZ2d0nq3tMtFeHM...|\n",
      "|DKRIko577clyWrs-U...|   0|2019-08-20 23:26:30|    0|KbJ_iruzUTCygusCp...|  5.0|I had an excellen...|     0|K1imL09UK3gQLpiLe...|\n",
      "|-IvBAqkaQcDt-fj-K...|   0|2020-02-26 16:20:44|    0|48xIp0tfX5yqMjSvd...|  3.0|Nice atmosphere, ...|     0|mNyYb6KcpzPYnac4w...|\n",
      "|ui4DfohP2wEh-BJbr...|   1|2021-08-05 20:31:29|    0|vm5MUnPFeusRhQHR7...|  5.0|Different Cheeses...|     2|vmUqcqMjlWoBM6qfm...|\n",
      "|yyYZjAQRfHuBh7TNW...|   0|2021-05-23 18:14:33|    0|WJPUQurPTf1zUlFVT...|  4.0|Delicious ice cre...|     0|GRPkECHl5GZmU-08-...|\n",
      "|_OMGZ3TXOfN2By7sk...|   0|2021-06-17 21:12:49|    0|ODtq3Ux8mQCYma9kH...|  1.0|We use to like go...|     0|NCBLMswqhW2-WFvXd...|\n",
      "|ro2ndorDn97HiFMRc...|   0|2020-04-16 18:49:58|    0|uBfK6qLCTKcs1Xe-t...|  1.0|BE AWARE! MUST RE...|     3|22eigmszeTGIRceXC...|\n",
      "|XQ6mIXhO9YjMKys2r...|   0|2021-07-20 23:19:16|    0|MwznHuHX2JK1va91Q...|  5.0|This place was re...|     0|EVkJEroYRozMeSwM3...|\n",
      "|HAXqgsSigoTyfwaiK...|   0|2020-10-09 16:41:47|    0|fg-f--CaaV81DaBuU...|  1.0|My fiancÃ© took 1 ...|     0|C6eybJ2JqrfwQ9zm3...|\n",
      "|U4X-tzwvTzW8uWxs2...|   0|2017-03-31 18:25:40|    0|zZ4A1W8-YZxlEizNo...|  5.0|Love this place. ...|     0|dYUNwoy7-l-cGQv9x...|\n",
      "|irGLBpffCInpt6ROH...|   0|2018-06-10 23:39:44|    0|JphW0nCsV5H0siwjb...|  5.0|Just wanted to sa...|     0|JMYvTAatAnGixdFfY...|\n",
      "|AqyC_6iGCTQGYGhUn...|   0|2018-04-22 09:55:08|    0|v3rrRtRPjMzQ6HE0r...|  5.0|This family-owned...|     0|WvbPRln_EWDxdvCsG...|\n",
      "|T9fF2OXS7FObKpmax...|   0|2021-06-16 16:14:13|    0|Ra3n0UJ9w8IcX_des...|  4.0|This spot has a c...|     0|QhpNHKk2Uby4cResl...|\n",
      "|1Vi_y09FXL9ayFkFn...|   0|2019-07-06 16:07:30|    0|CUrKXAuBkMUwPx0hk...|  5.0|This nail shop is...|     0|62VrY9E9ak1mUaaHt...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# from sparknlp.base import DocumentAssembler, Finisher\n",
    "# from sparknlp.annotator import Tokenizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = review_sample.select('text', 'stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Includes: tokenizer, normalizer, and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/12 02:30:06 WARN org.apache.spark.sql.SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkNLP components\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\").setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"normalized\"]).setOutputCol(\"lemmatized\")\n",
    "finisher = Finisher().setInputCols(['lemmatized']).setOutputCols([\"finished\"])\n",
    "\n",
    "# Create a SparkNLP pipeline\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer, lemmatizer, finisher])\n",
    "\n",
    "\n",
    "\n",
    "# Fit and transform the data\n",
    "processed_data = pipeline.fit(review_sample).transform(review_sample)\n",
    "\n",
    "# processed_data = processed_data.withColumn(\"normalized_tokens\", F.expr(\"transform(normalized, x -> x.result)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data.drop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|stars|            finished|\n",
      "+-----+--------------------+\n",
      "|  5.0|[the, experience,...|\n",
      "|  5.0|[it, truly, be, a...|\n",
      "|  1.0|[20, a, piece, fo...|\n",
      "|  4.0|[i, go, to, caffe...|\n",
      "|  1.0|[disgusting, cust...|\n",
      "|  1.0|[this, be, what, ...|\n",
      "|  1.0|[this, be, not, a...|\n",
      "|  5.0|[i, have, an, exc...|\n",
      "|  3.0|[nice, atmosphere...|\n",
      "|  5.0|[different, chees...|\n",
      "|  4.0|[delicious, ice, ...|\n",
      "|  1.0|[we, use, to, lik...|\n",
      "|  1.0|[be, aware, must,...|\n",
      "|  5.0|[this, place, be,...|\n",
      "|  1.0|[i, fianc, take, ...|\n",
      "|  5.0|[love, this, plac...|\n",
      "|  5.0|[just, want, to, ...|\n",
      "|  5.0|[this, familyowne...|\n",
      "|  4.0|[this, spot, have...|\n",
      "|  5.0|[this, nail, shop...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/12 02:30:35 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/08/12 02:30:35 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "# Use Word2Vec to convert text into dense vectors\n",
    "word2vec = Word2Vec(vectorSize=100, inputCol=\"finished\", outputCol=\"features\")\n",
    "word2vec_model = word2vec.fit(processed_data)\n",
    "word2vec_data = word2vec_model.transform(processed_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = word2vec_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/12 02:30:36 WARN org.apache.spark.ml.util.Instrumentation: [24ca7732] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/08/12 02:30:38 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/08/12 02:30:38 WARN com.github.fommil.netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "[Stage 27:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 1.2908750900615398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize and train the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"stars\")\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"stars\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20006"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
