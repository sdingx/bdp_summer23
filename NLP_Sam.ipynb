{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.appName('YelpML').getOrCreate()\n",
    "\n",
    "#change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "\n",
    "#print spark configuration settings\n",
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/07 01:30:26 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "dataDir = \"gs://msca-bdp-student-gcs/group2/yelp-datasample2\"\n",
    "business = spark.read.json(dataDir + \"/sample_business\")\n",
    "checkin = spark.read.json(dataDir + \"/sample_checkin\")\n",
    "review = spark.read.json(dataDir + \"/sample_review\")\n",
    "tip = spark.read.json(dataDir + \"/sample_tip\")\n",
    "user = spark.read.json(dataDir + \"/sample_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='arKiXax3ScSM_z3O-0CIyw', cool=0, date='2010-10-17 01:50:46', funny=0, review_id='zCNdcNrkIKefTPbak7CHVA', stars=5.0, text='Great Italian food!!  We have eaten here several times now and each time we have eaten something different.  Everytime the food has been fabulous!  We actually crave their food during the week and want to head over to Philadelphia for our Spasso food fix!', useful=0, user_id='bz2FrqfKrVmS7WwC-7C9aA')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "|business_id|cool|date|funny|review_id|stars|text|useful|user_id|\n",
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "|          0|   0|   0|    0|        0|    0|   0|     0|      0|\n",
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "review.select([count(when(review[c].isNull(), c)).alias(c) for c in review.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = review.sample(fraction=0.02, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|E03HQDIBBR1UHVd0B...|   0|2020-06-04 17:25:44|    0|Vnfrt7BhWVwBH4Zjq...|  5.0|The experience at...|     0|V8dN1Nvj8bKJ2GL8C...|\n",
      "|8_VaLzyX-H0nzbFIK...|   1|2014-12-03 00:25:46|    0|vJvjgW8cYExb2VM1o...|  5.0|It truly is as go...|     1|BkMqpJikNc3r5itc-...|\n",
      "|fTZZih-F-0VPbnv7H...|   0|2020-07-06 05:41:35|    0|QkSPQf4YCPg1Fs8s5...|  1.0|$20 a piece for a...|     0|G7Sabx-ak70f_Zt8O...|\n",
      "|Mfss88nOCGdyHkZZC...|   0|2014-03-07 23:37:04|    0|W8xEZ7SuEEaFwMdR6...|  4.0|I went to Caffe N...|     2|8MkZ6bpdP7x8Vlm_u...|\n",
      "|FgnNJt32BcXz7u4Ny...|   0|2020-06-10 16:43:40|    0|SJX5aSGfGJqt8PB5e...|  1.0|Disgusting custom...|     1|nVis9KXdsSyXLHFgJ...|\n",
      "|iwyMYJxTnwKLKBlOi...|   0|2021-04-14 00:23:51|    0|dsjGDOOJJdMKp7PvC...|  1.0|This is what I ge...|     1|ZHfo-Wq1DCcKvnsis...|\n",
      "|2BMk_drsikKWslJCX...|   0|2019-03-16 16:22:27|    0|lGGbrVGMbH2Ao2oGt...|  1.0|This was not a go...|     0|kMTZ2d0nq3tMtFeHM...|\n",
      "|DKRIko577clyWrs-U...|   0|2019-08-20 23:26:30|    0|KbJ_iruzUTCygusCp...|  5.0|I had an excellen...|     0|K1imL09UK3gQLpiLe...|\n",
      "|-IvBAqkaQcDt-fj-K...|   0|2020-02-26 16:20:44|    0|48xIp0tfX5yqMjSvd...|  3.0|Nice atmosphere, ...|     0|mNyYb6KcpzPYnac4w...|\n",
      "|ui4DfohP2wEh-BJbr...|   1|2021-08-05 20:31:29|    0|vm5MUnPFeusRhQHR7...|  5.0|Different Cheeses...|     2|vmUqcqMjlWoBM6qfm...|\n",
      "|yyYZjAQRfHuBh7TNW...|   0|2021-05-23 18:14:33|    0|WJPUQurPTf1zUlFVT...|  4.0|Delicious ice cre...|     0|GRPkECHl5GZmU-08-...|\n",
      "|_OMGZ3TXOfN2By7sk...|   0|2021-06-17 21:12:49|    0|ODtq3Ux8mQCYma9kH...|  1.0|We use to like go...|     0|NCBLMswqhW2-WFvXd...|\n",
      "|ro2ndorDn97HiFMRc...|   0|2020-04-16 18:49:58|    0|uBfK6qLCTKcs1Xe-t...|  1.0|BE AWARE! MUST RE...|     3|22eigmszeTGIRceXC...|\n",
      "|XQ6mIXhO9YjMKys2r...|   0|2021-07-20 23:19:16|    0|MwznHuHX2JK1va91Q...|  5.0|This place was re...|     0|EVkJEroYRozMeSwM3...|\n",
      "|HAXqgsSigoTyfwaiK...|   0|2020-10-09 16:41:47|    0|fg-f--CaaV81DaBuU...|  1.0|My fianc√© took 1 ...|     0|C6eybJ2JqrfwQ9zm3...|\n",
      "|U4X-tzwvTzW8uWxs2...|   0|2017-03-31 18:25:40|    0|zZ4A1W8-YZxlEizNo...|  5.0|Love this place. ...|     0|dYUNwoy7-l-cGQv9x...|\n",
      "|irGLBpffCInpt6ROH...|   0|2018-06-10 23:39:44|    0|JphW0nCsV5H0siwjb...|  5.0|Just wanted to sa...|     0|JMYvTAatAnGixdFfY...|\n",
      "|AqyC_6iGCTQGYGhUn...|   0|2018-04-22 09:55:08|    0|v3rrRtRPjMzQ6HE0r...|  5.0|This family-owned...|     0|WvbPRln_EWDxdvCsG...|\n",
      "|T9fF2OXS7FObKpmax...|   0|2021-06-16 16:14:13|    0|Ra3n0UJ9w8IcX_des...|  4.0|This spot has a c...|     0|QhpNHKk2Uby4cResl...|\n",
      "|1Vi_y09FXL9ayFkFn...|   0|2019-07-06 16:07:30|    0|CUrKXAuBkMUwPx0hk...|  5.0|This nail shop is...|     0|62VrY9E9ak1mUaaHt...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = review_sample.select('text', 'stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`normalized`' given input columns: [stars, text]; line 1 pos 10;\n'Project [text#59, stars#58, 'transform('normalized, lambdafunction(lambda 'x.result, lambda 'x, false)) AS normalized_tokens#539]\n+- Project [text#59, stars#58]\n   +- Sample 0.0, 0.02, false, 42\n      +- Relation[business_id#53,cool#54L,date#55,funny#56L,review_id#57,stars#58,text#59,useful#60L,user_id#61] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a SparkNLP pipeline\u001b[39;00m\n\u001b[1;32m      7\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[document_assembler, tokenizer, normalizer])\n\u001b[0;32m----> 9\u001b[0m review_sample \u001b[38;5;241m=\u001b[39m \u001b[43mreview_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalized_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransform(normalized, x -> x.result)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fit and transform the data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mfit(review_sample)\u001b[38;5;241m.\u001b[39mtransform(review_sample)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:2455\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2452\u001b[0m \n\u001b[1;32m   2453\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`normalized`' given input columns: [stars, text]; line 1 pos 10;\n'Project [text#59, stars#58, 'transform('normalized, lambdafunction(lambda 'x.result, lambda 'x, false)) AS normalized_tokens#539]\n+- Project [text#59, stars#58]\n   +- Sample 0.0, 0.02, false, 42\n      +- Relation[business_id#53,cool#54L,date#55,funny#56L,review_id#57,stars#58,text#59,useful#60L,user_id#61] json\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkNLP components\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
    "\n",
    "# Create a SparkNLP pipeline\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer])\n",
    "\n",
    "review_sample = review_sample.withColumn(\"normalized_tokens\", F.expr(\"transform(normalized, x -> x.result)\"))\n",
    "\n",
    "# Fit and transform the data\n",
    "processed_data = pipeline.fit(review_sample).transform(review_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"The experience at Jade is almost like a spa mixed in with a salon. You get a hot towel to put around your neck while your hair is being washed. Also this is the only salon I've been to where they do something called a Wet Strand Test. The stylist took a piece of my hair and was able to test it to see exactly what the hair needed in order to be healthy again. The atmosphere in the salon is filled with light and love and I really enjoyed my experience.\", stars=5.0, document=[Row(annotatorType='document', begin=0, end=453, result=\"The experience at Jade is almost like a spa mixed in with a salon. You get a hot towel to put around your neck while your hair is being washed. Also this is the only salon I've been to where they do something called a Wet Strand Test. The stylist took a piece of my hair and was able to test it to see exactly what the hair needed in order to be healthy again. The atmosphere in the salon is filled with light and love and I really enjoyed my experience.\", metadata={'sentence': '0'}, embeddings=[])], token=[Row(annotatorType='token', begin=0, end=2, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=13, result='experience', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=16, result='at', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=21, result='Jade', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=23, end=24, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=31, result='almost', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=36, result='like', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=38, end=38, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=40, end=42, result='spa', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=44, end=48, result='mixed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=50, end=51, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=53, end=56, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=58, end=58, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=60, end=64, result='salon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=65, end=65, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=67, end=69, result='You', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=71, end=73, result='get', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=75, end=75, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=77, end=79, result='hot', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=81, end=85, result='towel', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=87, end=88, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=90, end=92, result='put', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=94, end=99, result='around', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=101, end=104, result='your', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=106, end=109, result='neck', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=111, end=115, result='while', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=117, end=120, result='your', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=122, end=125, result='hair', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=127, end=128, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=130, end=134, result='being', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=136, end=141, result='washed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=142, end=142, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=144, end=147, result='Also', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=149, end=152, result='this', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=154, end=155, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=157, end=159, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=161, end=164, result='only', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=166, end=170, result='salon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=172, end=175, result=\"I've\", metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=177, end=180, result='been', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=182, end=183, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=185, end=189, result='where', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=191, end=194, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=196, end=197, result='do', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=199, end=207, result='something', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=209, end=214, result='called', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=216, end=216, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=218, end=220, result='Wet', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=222, end=227, result='Strand', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=229, end=232, result='Test', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=233, end=233, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=235, end=237, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=239, end=245, result='stylist', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=247, end=250, result='took', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=252, end=252, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=254, end=258, result='piece', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=260, end=261, result='of', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=263, end=264, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=266, end=269, result='hair', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=271, end=273, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=275, end=277, result='was', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=279, end=282, result='able', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=284, end=285, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=287, end=290, result='test', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=292, end=293, result='it', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=295, end=296, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=298, end=300, result='see', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=302, end=308, result='exactly', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=310, end=313, result='what', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=315, end=317, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=319, end=322, result='hair', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=324, end=329, result='needed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=331, end=332, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=334, end=338, result='order', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=340, end=341, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=343, end=344, result='be', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=346, end=352, result='healthy', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=354, end=358, result='again', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=359, end=359, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=361, end=363, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=365, end=374, result='atmosphere', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=376, end=377, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=379, end=381, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=383, end=387, result='salon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=389, end=390, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=392, end=397, result='filled', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=399, end=402, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=404, end=408, result='light', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=410, end=412, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=414, end=417, result='love', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=419, end=421, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=423, end=423, result='I', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=425, end=430, result='really', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=432, end=438, result='enjoyed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=440, end=441, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=443, end=452, result='experience', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=453, end=453, result='.', metadata={'sentence': '0'}, embeddings=[])], normalized=[Row(annotatorType='token', begin=0, end=2, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=13, result='experience', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=16, result='at', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=21, result='Jade', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=23, end=24, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=26, end=31, result='almost', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=36, result='like', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=38, end=38, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=40, end=42, result='spa', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=44, end=48, result='mixed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=50, end=51, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=53, end=56, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=58, end=58, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=60, end=64, result='salon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=67, end=69, result='You', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=71, end=73, result='get', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=75, end=75, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=77, end=79, result='hot', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=81, end=85, result='towel', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=87, end=88, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=90, end=92, result='put', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=94, end=99, result='around', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=101, end=104, result='your', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=106, end=109, result='neck', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=111, end=115, result='while', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=117, end=120, result='your', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=122, end=125, result='hair', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=127, end=128, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=130, end=134, result='being', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=136, end=141, result='washed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=144, end=147, result='Also', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=149, end=152, result='this', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=154, end=155, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=157, end=159, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=161, end=164, result='only', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=166, end=170, result='salon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=172, end=174, result='Ive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=177, end=180, result='been', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=182, end=183, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=185, end=189, result='where', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=191, end=194, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=196, end=197, result='do', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=199, end=207, result='something', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=209, end=214, result='called', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=216, end=216, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=218, end=220, result='Wet', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=222, end=227, result='Strand', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=229, end=232, result='Test', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=235, end=237, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=239, end=245, result='stylist', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=247, end=250, result='took', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=252, end=252, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=254, end=258, result='piece', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=260, end=261, result='of', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=263, end=264, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=266, end=269, result='hair', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=271, end=273, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=275, end=277, result='was', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=279, end=282, result='able', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=284, end=285, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=287, end=290, result='test', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=292, end=293, result='it', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=295, end=296, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=298, end=300, result='see', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=302, end=308, result='exactly', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=310, end=313, result='what', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=315, end=317, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=319, end=322, result='hair', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=324, end=329, result='needed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=331, end=332, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=334, end=338, result='order', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=340, end=341, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=343, end=344, result='be', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=346, end=352, result='healthy', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=354, end=358, result='again', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=361, end=363, result='The', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=365, end=374, result='atmosphere', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=376, end=377, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=379, end=381, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=383, end=387, result='salon', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=389, end=390, result='is', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=392, end=397, result='filled', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=399, end=402, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=404, end=408, result='light', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=410, end=412, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=414, end=417, result='love', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=419, end=421, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=423, end=423, result='I', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=425, end=430, result='really', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=432, end=438, result='enjoyed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=440, end=441, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=443, end=452, result='experience', metadata={'sentence': '0'}, embeddings=[])])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column text must be of type equal to one of the following types: [array<string>, array<string>] but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use Word2Vec to convert text into dense vectors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m Word2Vec(vectorSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m word2vec_data \u001b[38;5;241m=\u001b[39m word2vec_model\u001b[38;5;241m.\u001b[39mtransform(processed_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column text must be of type equal to one of the following types: [array<string>, array<string>] but was actually of type string."
     ]
    }
   ],
   "source": [
    "# Use Word2Vec to convert text into dense vectors\n",
    "word2vec = Word2Vec(vectorSize=100, inputCol=\"normalized\", outputCol=\"features\")\n",
    "word2vec_model = word2vec.fit(processed_data)\n",
    "word2vec_data = word2vec_model.transform(processed_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = word2vec_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"rating\")\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
