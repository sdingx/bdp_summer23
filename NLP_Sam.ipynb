{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Spark session\n",
    "spark = SparkSession.builder.appName('YelpML').getOrCreate()\n",
    "\n",
    "#change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "\n",
    "#print spark configuration settings\n",
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 20:46:03 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "dataDir = \"gs://msca-bdp-student-gcs/group2/yelp-datasample2\"\n",
    "business = spark.read.json(dataDir + \"/sample_business\")\n",
    "checkin = spark.read.json(dataDir + \"/sample_checkin\")\n",
    "review = spark.read.json(dataDir + \"/sample_review\")\n",
    "tip = spark.read.json(dataDir + \"/sample_tip\")\n",
    "user = spark.read.json(dataDir + \"/sample_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='arKiXax3ScSM_z3O-0CIyw', cool=0, date='2010-10-17 01:50:46', funny=0, review_id='zCNdcNrkIKefTPbak7CHVA', stars=5.0, text='Great Italian food!!  We have eaten here several times now and each time we have eaten something different.  Everytime the food has been fabulous!  We actually crave their food during the week and want to head over to Philadelphia for our Spasso food fix!', useful=0, user_id='bz2FrqfKrVmS7WwC-7C9aA')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "|business_id|cool|date|funny|review_id|stars|text|useful|user_id|\n",
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "|          0|   0|   0|    0|        0|    0|   0|     0|      0|\n",
      "+-----------+----+----+-----+---------+-----+----+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "review.select([count(when(review[c].isNull(), c)).alias(c) for c in review.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = review.sample(fraction=0.02, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1482"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|E03HQDIBBR1UHVd0B...|   0|2020-06-04 17:25:44|    0|Vnfrt7BhWVwBH4Zjq...|  5.0|The experience at...|     0|V8dN1Nvj8bKJ2GL8C...|\n",
      "|8_VaLzyX-H0nzbFIK...|   1|2014-12-03 00:25:46|    0|vJvjgW8cYExb2VM1o...|  5.0|It truly is as go...|     1|BkMqpJikNc3r5itc-...|\n",
      "|fTZZih-F-0VPbnv7H...|   0|2020-07-06 05:41:35|    0|QkSPQf4YCPg1Fs8s5...|  1.0|$20 a piece for a...|     0|G7Sabx-ak70f_Zt8O...|\n",
      "|Mfss88nOCGdyHkZZC...|   0|2014-03-07 23:37:04|    0|W8xEZ7SuEEaFwMdR6...|  4.0|I went to Caffe N...|     2|8MkZ6bpdP7x8Vlm_u...|\n",
      "|FgnNJt32BcXz7u4Ny...|   0|2020-06-10 16:43:40|    0|SJX5aSGfGJqt8PB5e...|  1.0|Disgusting custom...|     1|nVis9KXdsSyXLHFgJ...|\n",
      "|iwyMYJxTnwKLKBlOi...|   0|2021-04-14 00:23:51|    0|dsjGDOOJJdMKp7PvC...|  1.0|This is what I ge...|     1|ZHfo-Wq1DCcKvnsis...|\n",
      "|2BMk_drsikKWslJCX...|   0|2019-03-16 16:22:27|    0|lGGbrVGMbH2Ao2oGt...|  1.0|This was not a go...|     0|kMTZ2d0nq3tMtFeHM...|\n",
      "|DKRIko577clyWrs-U...|   0|2019-08-20 23:26:30|    0|KbJ_iruzUTCygusCp...|  5.0|I had an excellen...|     0|K1imL09UK3gQLpiLe...|\n",
      "|-IvBAqkaQcDt-fj-K...|   0|2020-02-26 16:20:44|    0|48xIp0tfX5yqMjSvd...|  3.0|Nice atmosphere, ...|     0|mNyYb6KcpzPYnac4w...|\n",
      "|ui4DfohP2wEh-BJbr...|   1|2021-08-05 20:31:29|    0|vm5MUnPFeusRhQHR7...|  5.0|Different Cheeses...|     2|vmUqcqMjlWoBM6qfm...|\n",
      "|yyYZjAQRfHuBh7TNW...|   0|2021-05-23 18:14:33|    0|WJPUQurPTf1zUlFVT...|  4.0|Delicious ice cre...|     0|GRPkECHl5GZmU-08-...|\n",
      "|_OMGZ3TXOfN2By7sk...|   0|2021-06-17 21:12:49|    0|ODtq3Ux8mQCYma9kH...|  1.0|We use to like go...|     0|NCBLMswqhW2-WFvXd...|\n",
      "|ro2ndorDn97HiFMRc...|   0|2020-04-16 18:49:58|    0|uBfK6qLCTKcs1Xe-t...|  1.0|BE AWARE! MUST RE...|     3|22eigmszeTGIRceXC...|\n",
      "|XQ6mIXhO9YjMKys2r...|   0|2021-07-20 23:19:16|    0|MwznHuHX2JK1va91Q...|  5.0|This place was re...|     0|EVkJEroYRozMeSwM3...|\n",
      "|HAXqgsSigoTyfwaiK...|   0|2020-10-09 16:41:47|    0|fg-f--CaaV81DaBuU...|  1.0|My fianc√© took 1 ...|     0|C6eybJ2JqrfwQ9zm3...|\n",
      "|U4X-tzwvTzW8uWxs2...|   0|2017-03-31 18:25:40|    0|zZ4A1W8-YZxlEizNo...|  5.0|Love this place. ...|     0|dYUNwoy7-l-cGQv9x...|\n",
      "|irGLBpffCInpt6ROH...|   0|2018-06-10 23:39:44|    0|JphW0nCsV5H0siwjb...|  5.0|Just wanted to sa...|     0|JMYvTAatAnGixdFfY...|\n",
      "|AqyC_6iGCTQGYGhUn...|   0|2018-04-22 09:55:08|    0|v3rrRtRPjMzQ6HE0r...|  5.0|This family-owned...|     0|WvbPRln_EWDxdvCsG...|\n",
      "|T9fF2OXS7FObKpmax...|   0|2021-06-16 16:14:13|    0|Ra3n0UJ9w8IcX_des...|  4.0|This spot has a c...|     0|QhpNHKk2Uby4cResl...|\n",
      "|1Vi_y09FXL9ayFkFn...|   0|2019-07-06 16:07:30|    0|CUrKXAuBkMUwPx0hk...|  5.0|This nail shop is...|     0|62VrY9E9ak1mUaaHt...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# from sparknlp.base import DocumentAssembler, Finisher\n",
    "# from sparknlp.annotator import Tokenizer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sample = review_sample.select('text', 'stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkNLP components\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\").setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols([\"normalized\"]).setOutputCol(\"lemmatized\")\n",
    "finisher = Finisher().setInputCols(['lemmatized']).setOutputCols([\"finished\"])\n",
    "\n",
    "# Create a SparkNLP pipeline\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer, lemmatizer, finisher])\n",
    "\n",
    "\n",
    "\n",
    "# Fit and transform the data\n",
    "processed_data = pipeline.fit(review_sample).transform(review_sample)\n",
    "\n",
    "# processed_data = processed_data.withColumn(\"normalized_tokens\", F.expr(\"transform(normalized, x -> x.result)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data.drop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|stars|            finished|\n",
      "+-----+--------------------+\n",
      "|  5.0|[the, experience,...|\n",
      "|  5.0|[it, truly, be, a...|\n",
      "|  1.0|[20, a, piece, fo...|\n",
      "|  4.0|[i, go, to, caffe...|\n",
      "|  1.0|[disgusting, cust...|\n",
      "|  1.0|[this, be, what, ...|\n",
      "|  1.0|[this, be, not, a...|\n",
      "|  5.0|[i, have, an, exc...|\n",
      "|  3.0|[nice, atmosphere...|\n",
      "|  5.0|[different, chees...|\n",
      "|  4.0|[delicious, ice, ...|\n",
      "|  1.0|[we, use, to, lik...|\n",
      "|  1.0|[be, aware, must,...|\n",
      "|  5.0|[this, place, be,...|\n",
      "|  1.0|[i, fianc, take, ...|\n",
      "|  5.0|[love, this, plac...|\n",
      "|  5.0|[just, want, to, ...|\n",
      "|  5.0|[this, familyowne...|\n",
      "|  4.0|[this, spot, have...|\n",
      "|  5.0|[this, nail, shop...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column text must be of type equal to one of the following types: [array<string>, array<string>] but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use Word2Vec to convert text into dense vectors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m Word2Vec(vectorSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m word2vec_data \u001b[38;5;241m=\u001b[39m word2vec_model\u001b[38;5;241m.\u001b[39mtransform(processed_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column text must be of type equal to one of the following types: [array<string>, array<string>] but was actually of type string."
     ]
    }
   ],
   "source": [
    "# Use Word2Vec to convert text into dense vectors\n",
    "word2vec = Word2Vec(vectorSize=100, inputCol=\"normalized\", outputCol=\"features\")\n",
    "word2vec_model = word2vec.fit(processed_data)\n",
    "word2vec_data = word2vec_model.transform(processed_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = word2vec_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"rating\")\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20006"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
